{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "01lcu_U-9v3r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiPPZE0N9wyH"
      },
      "source": [
        "1. What are channels and Kernels ?\n",
        "Ans:\n",
        "Channels are a group/layer of a particlular type of information. For example in an image of text, if we want to highlight all 'K's, then a layer that consists only 'K' is a channel. \n",
        "Kernels is a matrix which helps separate the channels, it is also called Feature Extractor.Every Channel has its own respective Kernel. \n",
        "2. Why should we (nearly always use 3X3 kernels?\n",
        "Ans: With smaller kernel sizes we get lower number of weights and more number of layers, for example, if we apply 3X3 kernel twice to get a final value, we will need (3X3+3X3) weights. If we apply 5X5 kernel once, we will need (5X5) weights. Due to higher number of weights, this is computationally expensive. Due to lower umber of layers, it learns simpler non linear features compared to 3X3. Hence smaller filter sizes are preferred over larger filter sizes. Odd sizes are prefered over even sizes, because with odd filters, all the previous layer pixels would be symmtrically around the output pixel. Without this symmetry, we will have to account for distortions across the layers. 1X1 is not used because the features extracted would be local with no consideration for the neighboring pixels. This makes 3X3, the popular option.\n",
        "3. How many times to we need to perform 3x3 convolutions operations to reach close to 1x1 from 199x199 (type each layer output like 199x199 > 197x197...)\n",
        "Ans: 99 times.  \n",
        "199X199\n",
        "197X197\n",
        "195X195\n",
        "193X193\n",
        "191X191\n",
        "189X189\n",
        "187X187\n",
        "185X185\n",
        "183X183\n",
        "181X181\n",
        "179X179\n",
        "177X177\n",
        "175X175\n",
        "173X173\n",
        "171X171\n",
        "169X169\n",
        "167X167\n",
        "165X165\n",
        "163X163\n",
        "161X161\n",
        "159X159\n",
        "157X157\n",
        "155X155\n",
        "153X153\n",
        "151X151\n",
        "149X149\n",
        "147X147\n",
        "145X145\n",
        "143X143\n",
        "141X141\n",
        "139X139\n",
        "137X137\n",
        "135X135\n",
        "133X133\n",
        "131X131\n",
        "129X129\n",
        "127X127\n",
        "125X125\n",
        "123X123\n",
        "121X121\n",
        "119X119\n",
        "117X117\n",
        "115X115\n",
        "113X113\n",
        "111X111\n",
        "109X109\n",
        "107X107\n",
        "105X105\n",
        "103X103\n",
        "101X101\n",
        "99X99\n",
        "97X97\n",
        "95X95\n",
        "93X93\n",
        "91X91\n",
        "89X89\n",
        "87X87\n",
        "85X85\n",
        "83X83\n",
        "81X81\n",
        "79X79\n",
        "77X77\n",
        "75X75\n",
        "73X73\n",
        "71X71\n",
        "69X69\n",
        "67X67\n",
        "65X65\n",
        "63X63\n",
        "61X61\n",
        "59X59\n",
        "57X57\n",
        "55X55\n",
        "53X53\n",
        "51X51\n",
        "49X49\n",
        "47X47\n",
        "45X45\n",
        "43X43\n",
        "41X41\n",
        "39X39\n",
        "37X37\n",
        "35X35\n",
        "33X33\n",
        "31X31\n",
        "29X29\n",
        "27X27\n",
        "25X25\n",
        "23X23\n",
        "21X21\n",
        "19X19\n",
        "17X17\n",
        "15X15\n",
        "13X13\n",
        "11X11\n",
        "9X9\n",
        "7X7\n",
        "5X5\n",
        "3X3\n",
        "1X1\n",
        "\n",
        "4. How are kernels initialized ?\n",
        "Ans:Kernels are initialized randomly. This is done to ensure that the variance of the output of a network layer stays bounded within reasonable limits instead of vanishing or exploding. the random values reduces the probability of lying in local minima. it also helps to move faster to the minimum value of cost function.\n",
        "5. What happens during the training of a DNN ?\n",
        "Ans: \n",
        "First we will start with values (often random) for the network parameters (wij weights and bj biases). b. Take the input data and pass them through the network to obtain their prediction. c. Compare these predictions obtained with the values of expected labels and calculate the loss with them. d. Perform the backpropagation in order to propagate this loss to each and every one of the weights that make up the model of the neural network. e. Use this propagated information to update the parameters of the neural network with the gradient descent in a way that the total loss is reduced and a better model is obtained. f. Continue iterating in the previous steps until we consider that we have a good model. Vanishing gradient problem is one of the key issue in DNN, there many fixes and workarounds have been proposed and investigated, such as alternate weight initialization schemes, unsupervised pre-training, layer-wise training, and variations on gradient descent. Perhaps the most common change is the use of the rectified linear activation function that has become the new default, instead of the hyperbolic tangent activation function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS4vGoWq2vfG"
      },
      "source": [
        ""
      ]
    }
  ]
}