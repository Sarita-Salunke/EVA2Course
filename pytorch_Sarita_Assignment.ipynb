{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D96Kbaze4D8n"
      },
      "source": [
        "# Reading and Transforming the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57R4-uv6CfAU"
      },
      "source": [
        "import torch\r\n",
        "import torchvision # provide access to datasets, models, transforms, utils, etc\r\n",
        "import torchvision.transforms as transforms\r\n",
        "\r\n",
        "train_set = torchvision.datasets.EMNIST(\r\n",
        "    root='./data'\r\n",
        "    ,train=True\r\n",
        "    ,download=True\r\n",
        "    ,split = \"balanced\"\r\n",
        "    ,transform=transforms.Compose([\r\n",
        "        transforms.ToTensor()\r\n",
        "    ])\r\n",
        ")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFr3r2TQ4MFm"
      },
      
      "source": [
        "# CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvwnrlbLDek5"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "class Network(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, padding = 1) #28*28* 1 > 28* 28 10 # There will be max pool after conv1. After Max pool the recepetive field will be 6*6\r\n",
        "        self.conv2 = nn.Conv2d(in_channels=10, out_channels= 10 , kernel_size=3, padding = 1) #14*14 *10 > 14 *14 10 # There will be max pool after conv2. After Max pool the recepetive field will be 16 *16\r\n",
        "        self.conv3 = nn.Conv2d(in_channels=10, out_channels= 20 , kernel_size=3) # 7 * 7 * 10 >  5 * 5 * 20 # Cov3 Receptive field 18\r\n",
        "        self.conv4 = nn.Conv2d(in_channels= 20, out_channels= 20 , kernel_size=3) # 5 *5 *20 > 3* 3* 20 # Cov4 Receptive field 20\r\n",
        "        self.conv5 = nn.Conv2d(in_channels= 20, out_channels= 30 , kernel_size=3) # 3* 3* 20 > 1 * 1 * 30 # Cov5 Receptive field 22\r\n",
        "        self.conv6 = nn.Conv2d(in_channels= 30, out_channels= 47 , kernel_size=1) # 3* 3* 20 > 1 * 1 * 30\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    def forward(self, t):\r\n",
        "      # (1) input layer\r\n",
        "      t = t\r\n",
        "       # (2) hidden conv layer\r\n",
        "      t = self.conv1(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      t = F.max_pool2d(t, kernel_size=2)\r\n",
        "\r\n",
        "        # (3) hidden conv layer\r\n",
        "      t = self.conv2(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      t = F.max_pool2d(t, kernel_size=2)\r\n",
        "       \r\n",
        "      # (4) hidden conv layer\r\n",
        "      t = self.conv3(t)\r\n",
        "      t = F.relu(t)\r\n",
        "       # (5) hidden conv layer  \r\n",
        "      t = self.conv4(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      # (6) hidden conv layer  \r\n",
        "      t = self.conv5(t)\r\n",
        "      t = F.relu(t)\r\n",
        "      # (6) hidden conv layer ; Streching the image to 47 channels  \r\n",
        "      t = self.conv6(t)\r\n",
        "      #make it 1d\r\n",
        "      t = t.view(-1, 47)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "      return F.log_softmax(t)\r\n",
        "      "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-0UGZID615W"
      },
      "source": [
        "# Function to check the prediction and actual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF3GOxtsvdNv"
      },
      "source": [
        "def get_num_correct(preds, labels):\r\n",
        "  return preds.argmax(dim=1).eq(labels).sum().item()"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqrl6dLX7MOD"
      },
      "source": [
        "#Initiate the model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsHJUXFU7P2B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-2s0IHiPDlo",
        "outputId": "06b426e8-f5a9-4731-bee2-fb67a13c555e"
      },
      "source": [
        "network = Network()\r\n",
        "print(network)\r\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Network(\n",
            "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(10, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(20, 20, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv5): Conv2d(20, 30, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv6): Conv2d(30, 47, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvJCtTGk7Xey"
      },
      "source": [
        "torch.set_grad_enabled(False)\r\n",
        "sample = next(iter(train_set)) \r\n",
        "image, label = sample\r\n",
        "image.shape, image.unsqueeze(0).shape\r\n",
        "pred = network(image.unsqueeze(0))\r\n",
        "pred.shape"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7fad30cdfac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAdwz9Aa7enE"
      },
      "source": [
        "# Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPxfiDlTt5-T",
        "outputId": "6e4b1784-1fea-4298-a145-0b588ab85781"
      },
      "source": [
        "import torch.optim as optim\r\n",
        "\r\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100)\r\n",
        "optimizer = optim.Adam(network.parameters(), lr=0.01)\r\n",
        "torch.set_grad_enabled(True)\r\n",
        "\r\n",
        "for epoch in range(20):\r\n",
        "\r\n",
        "    total_loss = 0\r\n",
        "    total_correct = 0\r\n",
        "\r\n",
        "    for batch in train_loader: # Get Batch\r\n",
        "        images, labels = batch \r\n",
        "\r\n",
        "        preds = network(images) # Pass Batch\r\n",
        "        loss = F.cross_entropy(preds, labels) # Calculate Loss\r\n",
        "\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward() # Calculate Gradients\r\n",
        "        optimizer.step() # Update Weights\r\n",
        "\r\n",
        "        total_loss += loss.item()\r\n",
        "        total_correct += get_num_correct(preds, labels)\r\n",
        "\r\n",
        "    print(\r\n",
        "        \"epoch\", epoch, \r\n",
        "        \"total_correct:\", total_correct, \r\n",
        "        \"loss:\", total_loss\r\n",
        "    )"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 0 total_correct: 64896 loss: 1563.7681303620338\n",
            "epoch 1 total_correct: 81388 loss: 975.7954006195068\n",
            "epoch 2 total_correct: 83347 loss: 910.7536614835262\n",
            "epoch 3 total_correct: 84506 loss: 876.2178189456463\n",
            "epoch 4 total_correct: 85027 loss: 859.2314097285271\n",
            "epoch 5 total_correct: 85209 loss: 849.9899033606052\n",
            "epoch 6 total_correct: 85491 loss: 838.4625252783298\n",
            "epoch 7 total_correct: 85635 loss: 832.7492441833019\n",
            "epoch 8 total_correct: 85715 loss: 828.8040691316128\n",
            "epoch 9 total_correct: 85957 loss: 825.1647506356239\n",
            "epoch 10 total_correct: 85908 loss: 821.4768992364407\n",
            "epoch 11 total_correct: 85877 loss: 822.9937447607517\n",
            "epoch 12 total_correct: 86229 loss: 813.6485145688057\n",
            "epoch 13 total_correct: 86443 loss: 804.8673409819603\n",
            "epoch 14 total_correct: 86487 loss: 801.6357666254044\n",
            "epoch 15 total_correct: 86683 loss: 793.6590873599052\n",
            "epoch 16 total_correct: 86618 loss: 794.7180162966251\n",
            "epoch 17 total_correct: 86673 loss: 792.8923965990543\n",
            "epoch 18 total_correct: 87037 loss: 783.9636635482311\n",
            "epoch 19 total_correct: 87083 loss: 780.6712603271008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhBd-yLuu-_q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
